# -*- coding: utf-8 -*-
"""DSML Project 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_WQsRJt51-VzunSwo2z7NBtQYfCvhWGQ
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, r2_score, mean_squared_error, classification_report
from sklearn import neighbors
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn import tree
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
import keras
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import math
from keras.models import Sequential
from keras.layers import Dense
from sklearn.cluster import KMeans
from datetime import datetime
import time
import statistics

f1  = pd.read_csv('stocknet_trn_data.csv')
f2 = pd.read_csv('stocknet_trn_data_targets.csv',header = None)
f2 = f2.drop([0], axis = 1)
f3 = pd.read_csv('stocknet_tst_data.csv')

f1.isna().any()

f2.columns

f3.columns

# Checking null value
f1.isnull().sum()

f2.isnull().sum()

f3.isnull().sum()

# Checking na value
f1.isna().any()

f2.isna().any()

f3.isna().any()

f1.dropna(inplace=True)

f2.dropna(inplace=True)

f3.dropna(inplace=True)

f1.isnull().sum()

f2.isnull().sum()

f3.isnull().sum()

fo1 = f1.copy()
fo3 = f3.copy()

nfo1 = fo1.copy()
nf2 = f2.copy()
nfo3 = fo3.copy()

f1["Trading Value"] = f1["Open"] * f1["Volume"] 
f1["HL Percent"] = (f1["High"] - f1["Low"]) / (f1["High"])

f3["Traded Value"] = f3["Open"] * f3["Volume"] 
f3["HL Percent"] = (f3["High"] - f3["Low"]) / (f3["High"])

"""High trading volume indicates strong interest and liquidity in the market, while low trading volume indicates the opposite.
The high-low percentage or H-L percent is a measure of volatility in the stock market. It is the difference between the highest and lowest prices of a stock for a given period, divided by the closing price for the same period. The H-L percent is a useful indicator of the level of price movement in the market. 
"""

f4 = f1.copy()
f4["Close"] = f2

f4

not_scaled_f1 = f1.copy()

f4

pcorr_matrix.columns = ['Open', 'High', 'Low', 'Volume','Trading Volume', 'HL percent', 'Close']
corr_matrix.index = ['Open', 'High', 'Low', 'Volume','Trading Volume', 'HL percent', 'Close']
df.corr()

scaler_fo1 = StandardScaler()
fo1 = scaler_fo1.fit_transform(fo1)

scaler_fo3 = StandardScaler()
fo3 = scaler_fo3.fit_transform(fo3)

scaler_f1 = StandardScaler()
f1 = scaler_f1.fit_transform(f1)

scaler_f2 = StandardScaler()
f2 = scaler_f2.fit_transform(f2)

scaler_f3 = StandardScaler()
f3 = scaler_f3.fit_transform(f3)

"""
New instances of the StandardScaler class, which is used to standardize data by removing the mean and scaling to unit variance.
The fit_transform method is called on each scaler object with the respective dataset as input (fo1, fo3, f1, f2, and f3). This standardizes the data for each dataset in place and stored back in their respective variables """

fo1

"""Metrics


"""

def RMSE(A,P):
  sum = 0
  for i in range(len(A)):
    sum = sum + (A[i] - P[i])**2
  error = math.sqrt((sum/len(A)))
  return error

def MAPE(A,P):
  sum = 0
  for i in range(len(A)):
     sum = sum+ abs((A[i] - P[i])/(A[i]))
  error = (sum / len(A))
  return error

def MBE(A,P):
  sum = 0
  for i in range(len(A)):
    sum = sum + abs(A[i] - P[i])
  error = sum/len(A)
  return error

def error_list(A,P,var):
  if var == 4:
    print("r2 score for 4 vars is: ", r2_score(A, P))
    print("RMSE score for 4 vars is: ", RMSE(np.array(A),np.array(P)))
    print("MAPE score for 4 vars is: ", MAPE(np.array(A),np.array(P)))
    print("MBE score for 4 vars is: ", MBE(np.array(A),np.array(P)))
  else:
    print("r2 score for 6 vars is: ", r2_score(A, P))
    print("RMSE score for 6 vars is: ", RMSE(np.array(A),np.array(P)))
    print("MAPE score for 6 vars is: ", MAPE(np.array(A),np.array(P)))
    print("MBE score for 6 vars is: ", MBE(np.array(A),np.array(P)))

"""Root Mean Squared Error (RMSE) is a measure of how much the predictions, represented by P, differ from the actual values, represented by A. 
 Mean Absolute Percentage Error (MAPE)  is a measure of how much the percentage difference between the predictions, represented by P, and the actual values, represented by A. 
 MBE is a measure of how much the predictions, represented by P, differ from the actual values, represented by A, on average.

Linear Regression
"""

fo_train, fo_test, f2_train, f2_test = train_test_split(fo1, f2, test_size = 0.2, random_state = 42)
linear_fo = LinearRegression().fit(fo_train, f2_train)
folr_pred = linear_fo.predict(fo_test)
error_list(f2_test, folr_pred,4)



f1_train, f1_test, f2_train, f2_test = train_test_split(f1, f2, test_size = 0.2, random_state = 42)
linear = LinearRegression().fit(f1_train, f2_train)
f2_pred = linear.predict(f1_test)
error_list(f2_test, f2_pred,6)

"""We perform two linear regression analyses on different sets of data and calculate the corresponding errors using the previously defined functions. 

error_list calls the error_list function to calculate and print the error metrics (RMSE, MAPE, MBE, and R2 Score) for the Linear Regression model using f1 features and f2 target data.




"""

kf = KFold(n_splits=10, shuffle=True, random_state=26)
mse_scores = cross_val_score(linear_fo, fo1, f2, cv=kf, scoring='r2')
mse_mean=np.mean(mse_scores)
print("A", mse_mean)

"""Ridge Regression"""

parameters = {'alpha':[1,0.1,0.01,0.001,0.0001,0],"fit_intercept": [True, False], 
              'solver': ['svd']}

Ridge_reg= GridSearchCV(Ridge(), param_grid = parameters,cv=10)
Ridge_reg.fit(fo_train,f2_train)
rr = Ridge_reg.best_estimator_
pred = rr.predict(fo_test)
error_list(f2_test, pred, 4)

Ridge_reg6= GridSearchCV(Ridge(), param_grid = parameters,cv=10)
Ridge_reg6.fit(f1_train,f2_train)
rr6 = Ridge_reg6.best_estimator_
pred6 = rr6.predict(f1_test)
error_list(f2_test, pred6, 6)

"""These hyperparameters include alpha which controls the regularization strength, fit_intercept which determines whether or not to include the intercept term in the model, and solver which is the solver for computing the ridge regression.

creates a GridSearchCV object Ridge_reg which is used to find the optimal values for the hyperparameters of the ridge regression. This object takes the Ridge model, the parameters dictionary, and cv=5 which is the number of cross-validation folds.

Lasso Regression
"""

param_lasso = {'alpha':[1,0.1,0.01,0.001,0.0001],"fit_intercept": [True, False]}

lasso_reg= GridSearchCV(Lasso(), param_grid = param_lasso,cv=5).fit(fo_train, f2_train)
lasso_pred = lasso_reg.predict(fo_test)
error_list(f2_test, pred, 4)


lasso_reg= GridSearchCV(Lasso(), param_grid = param_lasso,cv=5).fit(f1_train, f2_train)
lasso_pred = lasso_reg.predict(f1_test)
error_list(f2_test, pred6, 6)

"""Lasso regression is a type of linear regression that performs L1 regularization, which adds a penalty term to the cost function of linear regression,which is the absolute value of the coefficients of the features, which causes some of the coefficients to become zero, thus performing feature selection.

We first define a dictionary 'param_lasso' that contains the hyperparameters alpha and fit_intercept that we want to tune using GridSearchCV. The alpha parameter controls the strength of regularization, and fit_intercept is a boolean parameter that indicates whether to include an intercept term in the regression equation.

We then use GridSearchCV to perform a search over the hyperparameters to find the best combination of hyperparameters that minimize the mean squared error (MSE) on a 5-fold cross-validation set.

Random Forest Regression
"""

from sklearn.ensemble import RandomForestRegressor
model_rfr = RandomForestRegressor(n_estimators = 30, random_state = 42).fit(fo_train,f2_train.ravel())
rt_pred = model_rfr.predict(fo_test)
error_list(f2_test, rt_pred,4)

"""Code imports the RandomForestRegressor class from the sklearn.ensemble module. Then it creates an instance of the class called model_rfr with the following parameters:

n_estimators: The number of trees in the forest, set to 30.
random_state: The seed value used by the random number generator, set to 42.
Next, the fit() method of the RandomForestRegressor instance is called on the training data fo_train and f2_train to train the model.

After the model is trained, the predict() method is used to make predictions on the test data fo_test. The predicted values are stored in the variable rt_pred.

KNN
"""

scaler = MinMaxScaler()
f1_scaled=pd.DataFrame(scaler.fit_transform(fo1),columns=['Open', 'High', 'Low', 'Volume'])
X_sctrain,X_sctest,Y_sctrain,Y_sctest= train_test_split(f1_scaled,f2,test_size=0.20,random_state=42)

rms=[]
K_neighbor=[i for i in range(1,20)]
for K in K_neighbor:
  model = neighbors.KNeighborsRegressor(n_neighbors = K)
  model.fit(fo_train,f2_train)
  pred=model.predict(fo_test)
  rms.append(math.sqrt(mean_squared_error(f2_test,pred)))
plt.plot(K_neighbor,rms)

"""It is performing a loop over a range of values for the number of neighbors (K) in a K-Nearest Neighbors (KNN) regression model. For each value of K, the code trains a KNN regression model on the training data (fo_train and f2_train), makes predictions on the test data (fo_test), calculates the root mean squared error (RMSE) between the actual and predicted values using the math.sqrt and mean_squared_error functions from the math and sklearn.metrics modules, respectively, and appends the RMSE to a list called rms.

After the loop, the code creates a line plot using matplotlib.pyplot to visualize the relationship between the number of neighbors and the RMSE. 
"""

rms_sc=[]
K_neighbor=[i for i in range(1,20)]
for K in K_neighbor:
  model = neighbors.KNeighborsRegressor(n_neighbors = K)
  model.fit(X_sctrain,Y_sctrain)
  pred=model.predict(X_sctest)
  rms_sc.append(math.sqrt(mean_squared_error(Y_sctest,pred)))
plt.plot(K_neighbor,rms_sc)

"""plot of root mean square (RMS) error versus the number of neighbors K used in a K-nearest neighbors regression model. The first block of code is using the fo_train and f2_train datasets to train the model and fo_test to test the model. The second block of code is using the X_sctrain and Y_sctrain datasets to train the model and X_sctest to test the model, where X_sctrain and X_sctest have been standardized using a scaler such as StandardScaler or MinMaxScaler.

For each value of K in the range of 1 to 20, the code trains a K-nearest neighbors regression model with that value of K, and then predicts the output variable (f2_test or Y_sctest) using the corresponding test dataset (fo_test or X_sctest). 
"""

model_knn = neighbors.KNeighborsRegressor(n_neighbors = 7)
model_knn.fit(X_sctrain,Y_sctrain)
pred_knn=model_knn.predict(X_sctest)
error_list(Y_sctest,pred_knn,6)

"""Now implementing K-Nearest Neighbors regression (KNN) to predict the target variable using the training data X_sctrain and Y_sctrain.

Taking output using best regressor.
"""

nfo_train, nfo_test, nf2_train, nf2_test = train_test_split(nfo1,nf2, test_size = 0.2, random_state = 0)
linear_fo = LinearRegression().fit(nfo_train, nf2_train)
prediction = linear_fo.predict(fo3)

dates = []
string = 'd'
for i in range(len(prediction)):
  s = string + str(i+1)
  dates.append(s)
dates = np.array(dates)
prediction = np.array(prediction)

Output1 = pd.DataFrame(dates, columns = None)
Output2 = pd.DataFrame(prediction, columns = None)
Output = pd.merge(Output1,Output2, left_index = True, right_index = True)

Output.to_csv('project_output.txt', header = None, index = None, sep = ',')

print(Output1, Output2)